# Overview

This project presents a deep learning approach to generating Shakespearean-style text using a Recurrent Neural Network (RNN) with Gated Recurrent Units (GRUs). The model was trained on Shakespeare's works to produce coherent and stylistically accurate text. Custom training loops were implemented to enhance performance, demonstrating the flexibility and power of GRU-based RNNs for sequential text generation tasks. Additionally, the project highlights the broader implications of using recurrent neural networks in creative and artistic applications

Text generation has become a focal point of Natural Language Processing (NLP) research. The ability to generate human-like text has applications ranging from chatbots and content creation to creative endeavors such as poetry and prose. This project explores the use of a GRU-based Recurrent Neural Network to generate text in the style of William Shakespeare. Shakespeare's works, known for their intricate language and poetic rhythm, present a unique challenge for NLP models (Shakespeare, The Complete Works of William Shakespeare).
Leveraging TensorFlow (TensorFlow Documentation, n.d.) and a custom training loop, the model demonstrates an ability to learn complex linguistic structures and generate  human-like text. The study highlights the importance of recurrent architectures, especially GRUs, in capturing long-term dependencies in sequential data. By focusing on Shakespearean text, this project not only pushes the boundaries of AI creativity but also sheds light on the potential of RNNs in emulating specific literary styles (Chung et al., 2014).

# Background

Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by maintaining a hidden state that evolves over time. This characteristic makes them particularly well-suited for tasks where context and order are crucial, such as text generation. Unlike feedforward neural networks, RNNs incorporate loops within their architecture, allowing information to persist over time. This enables them to process input sequences of variable lengths and maintain contextual understanding throughout the sequence (Hochreiter & Schmidhuber, 1997).
Despite their advantages, traditional RNNs suffer from challenges such as vanishing and exploding gradients, which limit their ability to capture long-term dependencies in data. These issues hinder the performance of standard RNNs on complex language tasks, necessitating the development of more advanced architectures.

# Text Generation with an RNN
 
This project demonstrates how to generate text using a character-based Recurrent Neural Network (RNN). We will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data, we aim to predict the next character in the sequence ("e"). Longer sequences of text can be generated by calling the model repeatedly.

## Project Objectives:

Generate text using RNN.
Create training examples and targets for text generation.
Build an RNN model for sequence generation using Keras Subclassing.
Create a text generator and evaluate the output.

## Sample Output:

```bash
QUEENE:  
I had thought thou hadst a Roman; for the oracle,  
Thus by All bids the man against the word,  
Which are so weak of care, by old care done;  
Your children were in your holy love,  
And the precipitation through the bleeding throne.  
  
BISHOP OF ELY:  
Marry, and will, my lord, to weep in such a one were prettiest;  
Yet now I was adopted heir  
Of the world's lamentable day,  
To watch the next way with his father with his face?  
  
ESCALUS:  
The cause why then we are all resolved more sons.  
  
VOLUMNIA:  
O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,  
And love and pale as any will to that word.  
  
QUEEN ELIZABETH:  
But how long have I heard the soul for this world,  
And show his hands of life be proved to stand.  
  
PETRUCHIO:  
I say he look'd on, if I must be content  
To stay him from the fatal of our country's bliss.  
His lordship pluck'd from this sentence then for prey,  
And then let us twain, being the moon,  
were she such a case as fills m  
```

While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider the following:

* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.

* The structure of the output resembles a play - blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.

* The model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.


## Technologies Used:

- Python3.10

- TensorFlow

- Numpy

- NLP

- RNN


## Getting Started
 
To get a local copy up and running follow these simple steps.

### Installation

1. Clone the repo
```bash
git clone https://github.com/Singhsh01/Text_Generation_usingRNN.git
```

2. Change the project directory
```bash
cd text-generation-RNN
```

3. Create a conda environment and install the required dependencies:
```bash
conda env create -f enviorment.yml
```

4. Activate the conda environment:
```bash
conda activate water-quality-env
```

5. Run Jupyter Notebook in the directory
```bash
jupyter notebook
```

## Contribution:

Contributions to this project are welcomed. If you're interested in improving or extending this text generation RNN, consider the following:

- Enhancing the model's training on larger text sequences to potentially improve coherence in generated text.

- Experimenting with different RNN architectures or hyperparameters to optimize text generation results.

- Implementing post-processing techniques to refine the generated text for better readability or context coherence.

- Adding additional datasets or texts to diversify the model's training for broader language generation capabilities.

- Feel free to fork this repository, make improvements, and create pull requests with your enhancements. All contributions are appreciated and encouraged!

## References 
1. Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. *Neural Computation, 9*(8), 1735â€“1780. 
2. Chung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. *arXiv preprint arXiv:1412.3555.*
3. Brownlee, J. (2017). Gentle Introduction to the Adam Optimization Algorithm for Deep Learning. *Machine Learning Mastery.*
4. TensorFlow Documentation: https://www.tensorflow.org
5. Shakespeare, W. *The Complete Works of William Shakespeare.*
