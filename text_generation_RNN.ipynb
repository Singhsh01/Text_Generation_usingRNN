{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvKWR3W5bhYI"
      },
      "source": [
        "# Text generation with an RNN\n",
        "\n",
        "Projext Objectives:\n",
        "\n",
        "- Generate text using RNN.\n",
        "- Creat a training examples and targets for text generation.\n",
        "- Build a RNN model for serquence generation using Keras Subclassing.\n",
        "- Create a text generator and evaluate the output.\n",
        "\n",
        "\n",
        "\n",
        "This projext demonstrates how to generate text using character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Bellow is a sample output when the model in this tutorial trained for 30 epochs and started with prompt \"Q\"\n",
        "\n",
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "\n",
        "While some of the sentences are grammatical, most odo not make sense. The model has not learned the meaning of words, but there are some things to consider.\n",
        "\n",
        "  * The  model is charcter-based. When training started the model did not know how to spell an English-word, or that words were even a unit of text.\n",
        "\n",
        "  * The strucuter of the outputresembles a play-blocks of text generally begin with a speaker name, in all the capital letters similar to the dataset.\n",
        "\n",
        "  * As demonstrated below, the model is trained on small batches of text (100 cahracters each), and is stil able to generate a longer sequence of text with coherent strucutre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pbaxk-QjR_r"
      },
      "source": [
        "## Setup\n",
        "Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QeJDXIbwifEo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKaebvTzke-G"
      },
      "source": [
        "### Download the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oU8tk4TxjpIP"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    'shakespeare.txt',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78gjtFMilg13"
      },
      "source": [
        "### Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWxzttpTkYCJ",
        "outputId": "c50d8bf9-7c97-4d43-f81f-0d50de6dd23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenght of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f\"Lenght of text: {len(text)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QF5qmu4l31O"
      },
      "source": [
        "Let's take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfylHBE4lanl",
        "outputId": "44d85c1e-56d0-4db6-9898-55199e804d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa915edKohNS"
      },
      "source": [
        "Let's see how many unique characters are in our documnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvHz2xTFpAId",
        "outputId": "c9ae7390-b150-4508-f548-4f982cad7123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArtvLS3Rpgx-"
      },
      "source": [
        "This number represents the tokens that the Neural Network will consume during training, and will be generating during use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3V7dXcGpNZV"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY_8YQ5Jp1wr"
      },
      "source": [
        "#### Vectorize the text\n",
        "Before training, we need to convert the strings to a numerical representation.\n",
        "\n",
        "Using `tf.keras.layers.StringLookup` layer can convert each character into an numeric ID, it just needs the text to be split into tokens first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxdWEjJtpU0e",
        "outputId": "89cf80d6-2f66-4226-e363-27d83a1a58d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
              " [b'n', b'a', b'f', b'e', b't', b's']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_text = ['stefan', 'nafets']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_text, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TGz_GkOqyMS"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layers, this will help us to tranform the character into numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Wpwzsuzrc0_"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab),\n",
        "    mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW0aeYVEr0WY"
      },
      "source": [
        "it converts from tokens to character ID's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBEOBgSvr7dd",
        "outputId": "dfed7edb-b388-49f0-bf39-6ac3a33b54e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[58, 59, 44, 45, 40, 53],\n",
              " [53, 40, 45, 44, 59, 58]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzCQR2U6sAbK"
      },
      "source": [
        "Since the goal of this project is to generate text, it will be important to invert this representation and recover human-readable strings from it. To achieve this we can use `tf.keras.layers.StringLookup(......, invert=True)`.\n",
        "\n",
        "NOTE: Here instade of pasing the original vocabulary henerated with `sorted(set(list))` use the `get_vocabulary()` method so the `tf.keras.layers.StringLookup` layer so that the `[UNK]` are set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aczLcFbZ1g8X"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(),\n",
        "    invert=True,\n",
        "    mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1Q_d027y99"
      },
      "source": [
        "This layer recovers the cahracters from the vectors of ID's, and returns them as a `tf.RaggedTensor` of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4zY3Qy62EkA",
        "outputId": "6a5a2682-dc9e-4008-92b1-034d8f03f1ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b's', b't', b'e', b'f', b'a', b'n'],\n",
              " [b'n', b'a', b'f', b'e', b't', b's']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR22YfoJ3mys"
      },
      "source": [
        "You can use `tf.strings.reduce_join()` to join the characters back into strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOHv84_82K6x",
        "outputId": "93632687-e722-4173-fa51-1579ec0e7cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'stefan', b'nafets'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oT3Euh2yDMtT"
      },
      "outputs": [],
      "source": [
        "# lets create a function that we can call later\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-WJ97BEDNPD"
      },
      "source": [
        "## The prediciton task\n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "This is the task that we training the model to perform. The input to the model will be sequence of characters, and you train the model to predict the output-the following character at each time step.\n",
        "\n",
        "Since RNN's maintain an internal state that depends on the prevours seen elements, given all the characters computed untill this moment, what is the next character?\n",
        "\n",
        "## Create training examples and targets\n",
        "\n",
        "Next devide the text into examples sequences. Each input sequence will containe `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets containe the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is 'Hello'. The input sequence would be 'Hell', and the target sequnce 'ello'.\n",
        "\n",
        "First use the `tf.data.Dataset.from_tensor_slices` fucntion to convert text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Dp6ZxJDNTP",
        "outputId": "68d2b431-5808-4f77-832b-e54b2b1e81db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BVFduXRrR9te"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EXquCTSSRFs",
        "outputId": "1183aeee-1a62-4f66-f237-a9be89a7b004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-22 00:34:01.493958: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "56zpdaBJSfkZ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsIPEFVaTXqz"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwB-JKsSTp1n",
        "outputId": "0402a03c-586f-4e62-8aab-fc316ca288b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-22 00:34:06.820399: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjCgTZ4KUJCM"
      },
      "source": [
        "Is easier to see what this is doing if you join the tokens beack into strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "#better\n",
        "for seq in sequences.take(5):\n",
        "    seq = tf.squeeze(seq)  # Remove unnecessary dimensions if needed\n",
        "    print(text_from_ids(seq).numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFsBhlVCUWVs",
        "outputId": "4ce913a7-b578-46f1-db3b-598ac239398d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-22 00:13:33.896746: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zezk9OoYUphl"
      },
      "source": [
        "For training we will need a dataset of `(input, label)` pairs. Where `input` and `label` are sequences. At each time stap the inpout is the current character and the label is the next character\n",
        "\n",
        "The following function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestamp:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uXCaYaeoW_NJ"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsBNfmi7YIOL",
        "outputId": "9ab1e1d6-c5a1-4265-89e6-9dede5075ae0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_input_target(list('Tensorflow'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwSpQ7u7YSFZ"
      },
      "source": [
        "We can see that in the frist row we have the input without the target and in the secound row we have the target without the label, which makes input label pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fNIMOA8aYqaW"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK7HjIBPZ98O",
        "outputId": "8e9acf83-edbf-4e93-d36c-a8aea96d78f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input  : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-22 00:34:23.911084: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print('Input  :', text_from_ids(input_example).numpy())\n",
        "  print('Target :', text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuis5J_chWg"
      },
      "source": [
        "#### Create training batches\n",
        "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzbXjv_Gc2Ww",
        "outputId": "d176e1e3-3768-4b5f-98c1-1823e3b09a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(63, 100), dtype=tf.int64, name=None), TensorSpec(shape=(63, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 63\n",
        "\n",
        "\"\"\"Buffer size to shuffle the dataset\n",
        "(TF data is designed to work wiht possobly infinite sequences,\n",
        "so it dosen't attempt to shuffle the entire sequence in memory. Instadfe\n",
        "it maintaines a buffer is which it shuffles elements)\"\"\"\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE) \\\n",
        "    .batch(BATCH_SIZE, drop_remainder=True) \\\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYBu3sEAdp13"
      },
      "source": [
        "## Build The Model\n",
        "\n",
        "This section defines the model as `keras.Model` sublcass.\n",
        "\n",
        "The model have the following layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup label that will map each character-ID to a vector with `embedding_dim` dimensions.\n",
        "* `tf.keras.layers.GRU`: A type of RNN with seize `units=rnn_units` (We can also use LSTM layer here)\n",
        "* `tf.keras.layers.Debse`: The output layer, with `vocab_size` outputs. it outputs one logit for each character in vocabulary. There are the log-likehood of each character accoriding to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rr3WxA5eahmR"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulart in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebPdL-nfL0z6"
      },
      "source": [
        "The class bellow does the following::\n",
        "  - We derive a class from the keras.Model\n",
        "  - The constructor is used to define the layers of the model\n",
        "  - We define the pass forward using the layers defined in the constructor\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define embedding, GRU, and Dense layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        # Apply embedding layer\n",
        "        x = self.embedding(inputs, training=training)\n",
        "\n",
        "        # Initialize the GRU state if it's None\n",
        "        if states is None:\n",
        "            batch_size = tf.shape(inputs)[0]  # Dynamically infer the batch size\n",
        "            states = tf.zeros((batch_size, self.gru.units))  # Initialize states as zeros\n",
        "\n",
        "        # Process inputs and states through the GRU layer\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "        # Apply Dense layer for final output\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        # Return the appropriate outputs based on return_state\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ziyS14rrMjzP"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIlCxkcgMj3P"
      },
      "source": [
        "For each character the model looks up the embedding, runs GRU one timestamp with the embedding as input, and applies the dnese layer to generate logits predicting the top-likelihhood of the next character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgMXiBgWMj6E"
      },
      "source": [
        "### Try The Model\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "Frist check the shape of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5WPE9fNMkEH",
        "outputId": "eaf58f98-385f-42b5-dde8-2e6f1283e4d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(63, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx0Cc3YLlR0f"
      },
      "source": [
        "In the above example for sequence length of the input is `100` but the model can be run on inputs of any length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REmzH00Wluzd",
        "outputId": "9c060022-11d3-4ba1-8543-5003058dd0ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"my_model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m63\u001b[0m, │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m1024\u001b[0m))                 │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)          │        \u001b[38;5;34m67,650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKBVI1e2lyd5"
      },
      "source": [
        "To get the actual prediction from the model we need to sample from the output distribution, to get actual character indicies. The distribution is defined by the logits over hte character vocabulary\n",
        "\n",
        "Note: It is important to sample from the distribution as taking the argmax of the distribution can easily get the model stuck in a loop\n",
        "\n",
        "Try it for the first example in the batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "s_IS_mYTlyb0"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    example_batch_predictions[0],\n",
        "    num_samples = 1\n",
        ")\n",
        "\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwFY1SLHlyZ_"
      },
      "source": [
        "This gives us, at each timestep of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-RsjLk6lyWk",
        "outputId": "28f1a1c4-82bc-4730-e686-976e4b23eb66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([55, 32, 50, 19, 28, 17, 31, 30,  5, 43, 15, 64, 18, 15,  9, 59, 47,\n",
              "       44, 51, 56, 19, 30, 38,  9, 19, 64, 29, 44,  3, 58, 37, 57, 55, 12,\n",
              "       24, 20, 12, 18,  1, 59,  3, 46, 52, 13, 63, 37, 17, 50, 16, 54, 20,\n",
              "       15, 53, 36, 30, 39, 17, 35, 36,  5, 37, 32,  9, 64,  5,  3,  8, 12,\n",
              "        3, 63, 43, 62, 65, 19, 49, 56, 45, 37, 31, 17, 15, 54, 18, 37, 42,\n",
              "       30, 63, 11, 11, 35, 57, 20, 63, 15, 58, 60, 22, 65, 48, 22])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll68rpuGlySv"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLr3FP3JnoPi",
        "outputId": "b967f035-b743-4c2b-b3c9-09c9e3a716f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            "b'y friend.\\n\\nAUTOLYCUS:\\nAdieu, sir.\\n\\nFLORIZEL:\\nO Perdita, what have we twain forgot!\\nPray you, a word.'\n",
            "Next Char Predictions:\n",
            "b'pSkFODRQ&dByEB.thelqFQY.FyPe!sXrp;KG;E\\nt!gm?xXDkCoGBnWQZDVW&XS.y&!-;!xdwzFjqfXRDBoEXcQx::VrGxBsuIziI'\n"
          ]
        }
      ],
      "source": [
        "print('Input:', text_from_ids(input_example_batch[0]).numpy(), sep='\\n')\n",
        "print('Next Char Predictions:', text_from_ids(sampled_indices).numpy(), sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA-_jtexua2S"
      },
      "source": [
        "From this output of the we can see that the model is untrained and don't understand the text and can't make accurate predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiXP97nqog9G"
      },
      "source": [
        "### Train The Model\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN statem and the input this time step, predict the class of the next character\n",
        "\n",
        "#### Attach an optimizer, and loss function\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss funciton works in this case because it is applied across the last dimension of the predictions\n",
        "\n",
        "Beacuse the model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Gn54YyjBpQ-X"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giUC8DTasMmE",
        "outputId": "e569e73d-60e9-4aab-c719-ec1c71c00557"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediciton shape: \n",
            "(63, 100, 66)\n",
            "# (batch_size, sequence_length, vocab_size\n",
            "New loss:         tf.Tensor(4.189311, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    'Prediciton shape: ',\n",
        "    example_batch_predictions.shape,\n",
        "    '# (batch_size, sequence_length, vocab_size',\n",
        "    sep='\\n'\n",
        ")\n",
        "print('New loss:        ', example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlUK-djCvi4_"
      },
      "source": [
        "A newly initialized model should't be to sure of itself, the output logits should all have similar magnitudes. To confirm this you can chekc that the exmponential of the mean loss is approximatly equal to the vocabulary size. A much higher loss means the model is sure of it's wrong answers and is badly initialized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHoMcOISwsnx",
        "outputId": "24de1fe7-3c57-46ad-93ce-51b5f3703ee4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65.97732"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFJZb8NUxC2O"
      },
      "source": [
        "Note: This value can from the examples it was evaluated on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "hiYqEwy7w9SE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwNogo9dxoj4"
      },
      "source": [
        "#### Configure checkpoints\n",
        "We will use `tf.keras.callbacks.ModelChekpoint` to ensure that checkpoints are saved during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.1325 - loss: 2.3020\n",
            "Epoch 2/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1284 - loss: 2.2915\n",
            "Epoch 3/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.1601 - loss: 2.2765\n",
            "Epoch 4/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.2184 - loss: 2.2396\n",
            "Epoch 5/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.2186 - loss: 2.1714\n",
            "Epoch 6/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.2709 - loss: 2.0813\n",
            "Epoch 7/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.2960 - loss: 1.9826\n",
            "Epoch 8/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3289 - loss: 1.8840\n",
            "Epoch 9/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.3776 - loss: 1.8012\n",
            "Epoch 10/10\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4047 - loss: 1.7286\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,300</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">74,112</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m63\u001b[0m)         │         \u001b[38;5;34m6,300\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_8 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m74,112\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">267,956</span> (1.02 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m267,956\u001b[0m (1.02 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">89,318</span> (348.90 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m89,318\u001b[0m (348.90 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">178,638</span> (697.81 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m178,638\u001b[0m (697.81 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create dummy data (replace with your actual dataset)\n",
        "X_train = np.random.randint(0, 100, size=(1000, 50))  # 1000 samples, each with 50 features (sequence length)\n",
        "y_train = np.random.randint(0, 10, size=(1000,))  # 1000 labels for classification (10 possible classes)\n",
        "\n",
        "# Ensure the directory for checkpoints exists\n",
        "checkpoint_path = './training_checkpoints'\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt_{epoch}.weights.h5\")  # Add .weights.h5 extension\n",
        "\n",
        "# Define the model (this should be replaced with your actual model)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=100, output_dim=63),  # Example input dimension, adjust as needed\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create the training dataset (you can replace this with your actual dataset)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.batch(32)  # Batch size of 32\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, \n",
        "    save_weights_only=True,  # Save only the weights\n",
        "    save_freq='epoch'  # Save checkpoint at the end of every epoch\n",
        ")\n",
        "\n",
        "# Train the model with the checkpoint callback\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,  # Number of epochs for training\n",
        "    callbacks=[checkpoint_callback]  # Pass the checkpoint callback\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CTZdbPuoxtsR"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './training_checkpoints'\n",
        "# Name of the checkpoint file with .weights.h5 extension\n",
        "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "# Create the ModelCheckpoint callback\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, \n",
        "    save_weights_only=True,  # Save only the weights\n",
        "    save_freq='epoch'  # Save checkpoint at the end of every epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eer326nAy1GB"
      },
      "source": [
        "## Fit the Model\n",
        "To keep training time resonable, uses 10 epochs to train the model.\n",
        "\n",
        "Note: Set the runtime to GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: (63, 100)\n",
            "Target shape: (63, 100)\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Input shape:\", input_example_batch.shape)\n",
        "    print(\"Target shape:\", target_example_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "# 2\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]  # All tokens except the last\n",
        "    target_text = chunk[1:]  # All tokens except the first\n",
        "    return input_text, tf.argmax(target_text, axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "# Apply the function to create input-target pairs\n",
        "def split_input_target(sequence):\n",
        "    \"\"\"\n",
        "    Splits input sequence into input and target pairs.\n",
        "    For example, if the input is \"hello\", the function returns\n",
        "    input: \"hell\" and target: \"ello\".\n",
        "    \"\"\"\n",
        "    input_text = sequence[:-1]  # All tokens except the last\n",
        "    target_text = sequence[1:]  # All tokens except the first\n",
        "    return input_text, target_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input example batch shape: (63, 63, 100)\n",
            "Model expects input shape: (None, 50)\n",
            "Reshaped input batch shape: (7938, 50)\n",
            "Output shape from model: (7938, 10)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-21 23:59:21.799685: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "# Inspect dataset shapes\n",
        "for input_example_batch, _ in dataset.take(1):\n",
        "    print(\"Input example batch shape:\", input_example_batch.shape)\n",
        "    print(\"Model expects input shape:\", model.input_shape)\n",
        "    # If needed, reshape the input to match model's expectation\n",
        "    if input_example_batch.shape[-1] != model.input_shape[-1]:\n",
        "        input_example_batch = tf.reshape(input_example_batch, (-1, *model.input_shape[1:]))\n",
        "        print(\"Reshaped input batch shape:\", input_example_batch.shape)\n",
        "\n",
        "    # Predict and verify\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\"Output shape from model:\", example_batch_predictions.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Input shape:\", input_example_batch.shape)\n",
        "    print(\"Target shape:\", target_example_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "# Step 1: Preprocessing the Data\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum length (50 tokens)\n",
        "MAX_LENGTH = 50\n",
        "\n",
        "# Pad and truncate the input data\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    print(\"Original Input shape:\", input_example_batch.shape)\n",
        "    print(\"Original Target shape:\", target_example_batch.shape)\n",
        "    \n",
        "    # Pad and truncate the input and target sequences to length 50\n",
        "    input_example_batch = pad_sequences(input_example_batch, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "    target_example_batch = pad_sequences(target_example_batch, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
        "    \n",
        "    print(\"Modified Input shape:\", input_example_batch.shape)\n",
        "    print(\"Modified Target shape:\", target_example_batch.shape)\n",
        "\n",
        "    print(\"Padded Input shape:\", input_example_batch.shape)\n",
        "    print(\"Padded Target shape:\", target_example_batch.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " # depreceted\n",
        "import time  # Standard Python library for timing\n",
        "\n",
        "# Step 2: Training the Model\n",
        "EPOCHS = 10\n",
        "\n",
        "# Start measuring time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "# Calculate the time taken\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "print(f\"Training completed in {execution_time:.2f} seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOeO1vlzznTy",
        "outputId": "4c489f76-b5b6-4f10-cc87-4959480122f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 919ms/step - loss: 3.0685\n",
            "Epoch 2/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 986ms/step - loss: 1.9097\n",
            "Epoch 3/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 1s/step - loss: 1.6235\n",
            "Epoch 4/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 1s/step - loss: 1.4765\n",
            "Epoch 5/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 1s/step - loss: 1.3892\n",
            "Epoch 6/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 2s/step - loss: 1.3249\n",
            "Epoch 7/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 1s/step - loss: 1.2786\n",
            "Epoch 8/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 1s/step - loss: 1.2314\n",
            "Epoch 9/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 1s/step - loss: 1.1862\n",
            "Epoch 10/10\n",
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 1s/step - loss: 1.1448\n",
            "CPU times: user 2h 32min 9s, sys: 19min 1s, total: 2h 51min 11s\n",
            "Wall time: 33min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8kUHGyP2yqb"
      },
      "source": [
        "## Generate Text\n",
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as we execute it.\n",
        "\n",
        "Each time we call the model we pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to countinue generating text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kbEpE7PJavZg"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybPv6YTka-1l"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the genereaated text, you'll see that model knows when to capitalize, make paragraps and imitates a Shakespare-like writing vocabulary. With the small number of training epochs, it has not yet learne to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHCxvpp1bynd",
        "outputId": "19910db4-9dcb-4a1c-c95c-36aa4f726a8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Back, let me see the earth, so much prepared\n",
            "In act mirthre spices fortes of bows;\n",
            "And, for not, and therefore, so. Now, brother\n",
            "Temple themselves, the promptune of the kind\n",
            "Of my person's brother's letter, pursued\n",
            "For she wash the hour. Come hither, you shall not: O, Warwick, we will\n",
            "one with this beauty; yet, to be found the cause\n",
            "Of his land a mother confix it on,\n",
            "We mird my strengthen blows out to confirm't.\n",
            "\n",
            "GLOUCESTER:\n",
            "My gracious left, for thou contagate a\n",
            "cow, i' Henrou to a whole designinity-rich palm\n",
            "For claim the people, even to my fortune;\n",
            "Cannot stand upon thy fiving Lord Aubilit,\n",
            "Here in piercing master lips. Of Henry!\n",
            "How fortune save an end o' knee, wrink'd\n",
            "Which thou wouldst kill'd her month-house-ruin\n",
            "Matcliff'd. Wile no offection and your good,\n",
            "When take he findless, make me fast, their sonows:\n",
            "A duelles, most perform like the day,\n",
            "Found lost her safely in the ground of ninmpel, have proposite\n",
            "To usure me for the gates of my heed;\n",
            "And, match Greet and my falon is th \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.1812810897827148\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkRXL_FJdOe_"
      },
      "source": [
        "The easies thing you can do to imporve the results is to trian it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "We can also experiment with diffenret start stirng, try adding another RNN layer to imporve the model's accuracy, or adjust the temperature parameter to generate more or less random predictons.\n",
        "\n",
        "If we want the model to generate text faster the easiest thing we can do is batch the next generation in the example below the model generates 5 output in about the same time it took to generate 1 above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBAZDqcQePDy",
        "outputId": "95ee6c2a-a15a-4c92-ff63-4f54182f7c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nHe's a foe intend to strengthen'd by a thousand death.\\nMy sleep'st thing of all three fentless have known\\nThou canst unfidectly, and hereway;\\nA business of the kinghom that same vow\\nLet me can apply him. Pray you, sir: I have needs must up.\\n\\nPEONTES:\\nUnless\\nI thank you off a wife:\\nThese great care, disonder pride me: we will come,\\nOr let's have counted. Would ye we heard you would,\\nThat common vialing forticulate hate the curious servant\\nA destinew and idle and a house:\\nBesides his hand. And whom take him, pardon' her!\\n\\nLord Mayor:\\nThy eyes sit would leave you!\\n\\nAUTOLYCUS:\\nAward, fair Juckins!\\n\\nBENVOLIO:\\nWouldst those countryman? is Some govern'd;\\nBe gvoice his own himself from him!\\n\\nFirst Citizen:\\nI must abbuly fire me to thy father.\\nWho dancles hopely the kites of all the world?\\n\\nGLOUCESTER:\\nAnd so stopp'd, add husian so too? There be\\nDid pardon alive.\\n\\nJULIET:\\nO seek there, he says love; how true loved good,\\nWhich you sir, yet it list, your kind kneel,\\nYet to be sit up in purpose.\"\n",
            " b\"ROMEO:\\nThat Clarence shall be our city in our queen.\\nRaughter, when it is the hair within my name?\\n\\nNor England's wife.\\n\\nLUCENTIO:\\nA shrewdly stepper, awaked of you:\\nOn, yee not is it is to make a deep of you\\nAnd yet be sent to know\\nFor what's agial; for the presty office.\\nThere is no need. I beseech you,--I say the tribbunipate:\\nHast thou there in the till starve-tandly knups\\nwith her advernal thither rather than as\\nSclack they valour; lay from the field upon\\nWhich women tria impedients, and so shall scarce\\nWith part for such mooimed and day it still.\\n\\nGLOUCESTER:\\n\\nKING RICHARD III:\\nFaith, be gone, no turn for Thee.\\n\\nCORIOLANUS:\\nYour haste have done.\\n\\nPETRUCHIO:\\nI unto your hand,\\nThat batter have empluck up the gate of Hereport\\nWith this new knaves of Forfess: and my sweet sed,\\nNear that hath affected him for you.\\n\\nROMEO:\\nGive he kill'd; my good lords, ballain,\\nYou shall great men: would yet unwartly, norf;\\nAnd every kiss be into when they he,\\nHollow'd the news. Come, my swif.\\n\\nJULIET:\\nO m\"\n",
            " b\"ROMEO:\\nGood gentle malicit,\\nWhen that reture pardon my kindrament;\\nOur venity too much, defended my eyes!\\nThen all usess of Edward's name; found with her.\\n\\nHENRY BOLINGBROKE:\\nGo, say, my master, that full't this is\\nOur courting with some sovereign? we'll have you not cold\\nThe next wayed and a way of honest thousand false:\\nFill'd you the man; you know not of my reven!\\nIn any misto spake the moan, if I can sleep\\nWe lough as my advantaged, and\\nthis fortunate love; too depart and smarral,\\nShall have soundly; fear these fearful crims\\nIf bearing any coming alucodam that laugh as frame.\\nWhen it is this bread--go to would not feel?\\nCall it out of hath spoke to dry your tongue,\\nAnd being something that are the second countel-shall.\\nHear me, thbut you be three. Common, methinks the\\nrevelute bear my first crept of my heart\\nTy'll say such a sleepy-doubt, say the days\\nI propared most giving, our commonicks\\nThan Rome shall hear us kind? why dost thou dost thou?\\n\\nFipsta death?\\n\\nRICHARD:\\nLord Ellam, and ev\"\n",
            " b\"ROMEO:\\nMust wise men ne'er the head of her cock.\\n\\nMENENIUS:\\nPeace, master, Buckingham; God so,\\nfeeling and signies do dance and such more\\nGod love him knows; covethers here do lose you,\\nLook for some overnee of his opprison,\\nReselve yor proper out the wazing.\\n\\nCOMINIUS:\\nSay, Neighbour: we'll do you now,\\nAufidiusl comfort Warwick them here beholder,\\nLooking for this roar'd fewful that raised me\\nHere, mark his head and make him intend o' the mind:\\nGive me her tume to shake the finder of the fire:\\nI have spoke to him, or what I shin.\\nGive me the chance.\\n\\nNurse:\\nI do bear me; for by him.\\n\\nISABELLA:\\nMore stop me lies, for it.\\n\\nLEONTES:\\nHast thou the air? swear mothy good, and let the best\\nThat justice wanders of the nightifies;\\nAt piercing us tybalt with tears, and so proin; 'tis defended\\nhim one, as over esump, the deep outrage: he\\nlittle fire, or shame it would pay now, we cannot death,\\nAnd so it deep sickly certain, I know,\\nOur knees have pity gently prepeased\\nThe father stand him cast and go\"\n",
            " b\"ROMEO:\\nWhat noise the roid?\\n\\nJOHN OF GAUNT:\\nGo to the city tell me to 'love:\\nNothing master, good Hortension, plunk in Sicil,\\nOn wrong'd upon thy soul but health; come hither,\\nThy husband made their birthest lords, thou set'st beguiled;\\nOne first thou never that must be spen?\\n\\nBAPTISTA:\\nNo so; let them ale, not too; for I hear you\\nin every officer. Affond, ho! neither, mar!\\n\\nNATHANRY:\\nI will be tell the coward exchange it;\\nAnd, for what fastish fight will make his palace: I, he be a\\nmoving beds, that will requies your grave.\\n\\nROMEO:\\nI desire it in the bird of leave you die.\\n\\nPALIG:\\nUt assist me, as you are, not first;\\nIf he did Richard Edward be here. So, let me know\\nBy Capitol; deny the small-weeping,\\nIf never here against up, Bruin have have kiss\\nthy blastering: you knew will our uncles\\nAmbitious starn upon his, presently up;\\nAgainst this a sin that you may knock her he.\\nHe estate the former landard pertack.\\n\\nHENRY VOLINGBROKE:\\nPluck'd by the belly ingression of a wish\\nFrom such the flash\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time 3.2184500694274902\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO', 'ROMEO', 'ROMEO', 'ROMEO', 'ROMEO' ])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + \"_\" * 80)\n",
        "print(\"\\nRun time\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-W2aUwafLQF"
      },
      "source": [
        "## Export the generator\n",
        "This signal-step nodel can easily be saved and restored, allowing you to use it anywhere a `tf.saved_model` is accepted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1VfBnmYfo-Z",
        "outputId": "9eee1a48-aa75-4853-c33f-4654e7a0dc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ],
      "source": [
        "# save the one step model\n",
        "tf.saved_model.save(one_step_model, 'one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "R72cmUybfph3"
      },
      "outputs": [],
      "source": [
        "# reload the model\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfQLT-xwrLcf",
        "outputId": "c940559c-aa18-4823-96a0-9de9d58fefe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "And yet he is left one hour that shall never say\n",
            "He must not make it so blind a humour.\n",
            "And you wi\n"
          ]
        }
      ],
      "source": [
        "states =  None\n",
        "next_char = tf.constant(['ROMEO'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states = states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIxQh3kVr3da"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but it does not give us much control, it uses teacher-forcing which prevents bad predicitons from being fed back to the model, so the model never learns to recover form mistakes.\n",
        "\n",
        "So now that we've seen how to run the model manualy next we can implement the tarining loop. This gives a starting point if, for example, we want to implement curriculum learning to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function\n",
        "\n",
        "We use `tf.GradientTape` to track the gradients. More about this approach :  [eager execution guide](https://www.tensorflow.org/guide/basics).\n",
        "\n",
        "The basic procedure is\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and aply them to the model using optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WyqgLJAXr3Zi"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAkW6KkI1Nrt"
      },
      "source": [
        "The above implementation of the `train_step` method follows `Keras train_step conventions`. This is optional, but it allows us to change the begavior of the train step and still use keras `Model.cmpile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "9gkMieOT2ZXx"
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units = rnn_units\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IcCkKXiU2yN9"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzIy9lLq3OET",
        "outputId": "001f2a4d-ad13-4f0b-c1f7-4aef8b84ad1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 988ms/step - loss: 2.4950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x174d977f0>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9Pbaxk-QjR_r",
        "78gjtFMilg13",
        "W3V7dXcGpNZV",
        "SY_8YQ5Jp1wr",
        "V-WJ97BEDNPD",
        "rNuis5J_chWg",
        "zYBu3sEAdp13",
        "VgMXiBgWMj6E",
        "qiXP97nqog9G",
        "XwNogo9dxoj4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
